{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN_project_MVA.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "lG8oa32OxO1-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**You may need to install [OpenCV](https://pypi.python.org/pypi/opencv-python) and [scikit-video](http://www.scikit-video.org/stable/).**"
      ]
    },
    {
      "metadata": {
        "id": "2-PF33IZxn-k",
        "colab_type": "code",
        "outputId": "117277f8-ac84-4f6b-c4d9-10a3fe78eb1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install scikit-video"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-video\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/a6/c69cad508139a342810ae46e946ebb3256aa6e42f690d901bb68f50582e3/scikit_video-1.1.11-py2.py3-none-any.whl (2.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.3MB 12.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from scikit-video) (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from scikit-video) (1.14.6)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from scikit-video) (4.0.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->scikit-video) (0.46)\n",
            "Installing collected packages: scikit-video\n",
            "Successfully installed scikit-video-1.1.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XpOlQOl4xO2A",
        "colab_type": "code",
        "outputId": "3c9faadf-4b2f-4ed6-983d-dfca12f74173",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "import skvideo.io\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "from keras.models import Sequential,model_from_json\n",
        "from keras.layers.core import Dense\n",
        "from keras.optimizers import sgd\n",
        "from keras.layers import Conv2D, MaxPooling2D, Activation, AveragePooling2D,Reshape,BatchNormalization, Flatten"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "xgIu2lnfSLBn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#set to True to launch every training\n",
        "training=False\n",
        "testing=True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8mV6Cl2fxO2F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# MiniProject #3: Deep Reinforcement Learning"
      ]
    },
    {
      "metadata": {
        "id": "OdZYa2drxO2F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__Notations__: $E_p$ is the expectation under probability $p$. Please justify each of your answer and widely comment your code."
      ]
    },
    {
      "metadata": {
        "id": "c6HMj3LyxO2H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Context"
      ]
    },
    {
      "metadata": {
        "id": "nQGRi3TqxO2I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In a reinforcement learning algorithm, we modelize each step $t$ as an action $a_t$ obtained from a state $s_t$, i.e. $\\{(a_{t},s_{t})_{t\\leq T}\\}$ having the Markov property. We consider a discount factor $\\gamma \\in [0,1]$ that ensures convergence. The goal is to find among all the policies $\\pi$, one that maximizes the expected reward:\n",
        "\n",
        "\\begin{equation*}\n",
        "R(\\pi)=\\sum_{t\\leq T}E_{p^{\\pi}}[\\gamma^t r(s_{t},a_{t})] \\> ,\n",
        "\\end{equation*}\n",
        "\n",
        "where: \n",
        "\\begin{equation*}p^{\\pi}(a_{0},a_{1},s_{1},...,a_{T},s_{T})=p(a_{0})\\prod_{t=1}^{T}\\pi(a_{t}|s_{t})p(s_{t+1}|s_{t},a_{t}) \\> .\n",
        "\\end{equation*}\n",
        "\n",
        "We note the $Q$-function:\n",
        "\n",
        "\\begin{equation*}Q^\\pi(s,a)=E_{p^{\\pi}}[\\sum_{t\\leq T}\\gamma^{t}r(s_{t},a_{t})|s_{0}=s,a_{0}=a] \\> .\n",
        "\\end{equation*}\n",
        "\n",
        "Thus, the optimal Q function is:\n",
        "\\begin{equation*}\n",
        "Q^*(s,a)=\\max_{\\pi}Q^\\pi(s,a) \\> .\n",
        "\\end{equation*}\n",
        "\n",
        "In this project, we will apply the deep reinforcement learning techniques to a simple game: an agent will have to learn from scratch a policy that will permit it maximizing a reward."
      ]
    },
    {
      "metadata": {
        "id": "OcjQdaDFxO2I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The environment, the agent and the game"
      ]
    },
    {
      "metadata": {
        "id": "K-e68ZAHxO2J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The environment"
      ]
    },
    {
      "metadata": {
        "id": "-1ilbUK0xO2L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "```Environment``` is an abstract class that represents the states, rewards, and actions to obtain the new state."
      ]
    },
    {
      "metadata": {
        "id": "u3wmcXSRxO2L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Environment(object):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def act(self, act):\n",
        "        \"\"\"\n",
        "        One can act on the environment and obtain its reaction:\n",
        "        - the new state\n",
        "        - the reward of the new state\n",
        "        - should we continue the game?\n",
        "\n",
        "        :return: state, reward, game_over\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reinitialize the environment to a random state and returns\n",
        "        the original state\n",
        "\n",
        "        :return: state\n",
        "        \"\"\"\n",
        "        pass\n",
        "    \n",
        "    def draw(self):\n",
        "        \"\"\"\n",
        "        Visualize in the console or graphically the current state\n",
        "        \"\"\"\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "somClDhvxO2P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The method ```act``` allows to act on the environment at a given state $s_t$ (stored internally), via action $a_t$. The method will return the new state $s_{t+1}$, the reward $r(s_{t},a_{t})$ and determines if $t\\leq T$ (*game_over*).\n",
        "\n",
        "The method ```reset``` simply reinitializes the environment to a random state $s_0$.\n",
        "\n",
        "The method ```draw``` displays the current state $s_t$ (this is useful to check the behavior of the Agent).\n",
        "\n",
        "We modelize $s_t$ as a tensor, while $a_t$ is an integer."
      ]
    },
    {
      "metadata": {
        "id": "xWkkkxIPxO2Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The Agent"
      ]
    },
    {
      "metadata": {
        "id": "opdVKxnWxO2R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The goal of the ```Agent``` is to interact with the ```Environment``` by proposing actions $a_t$ obtained from a given state $s_t$ to attempt to maximize its __reward__ $r(s_t,a_t)$. We propose the following abstract class:"
      ]
    },
    {
      "metadata": {
        "id": "9wVr8vsexO2U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Agent(object):\n",
        "    def __init__(self, epsilon=0.1, n_action=4):\n",
        "        self.epsilon = epsilon\n",
        "        self.n_action = n_action\n",
        "    \n",
        "    def set_epsilon(self,e):\n",
        "        self.epsilon = e\n",
        "\n",
        "    def act(self,s,train=True):\n",
        "        \"\"\" This function should return the next action to do:\n",
        "        an integer between 0 and 4 (not included) with a random exploration of epsilon\"\"\"\n",
        "        if train :\n",
        "          if np.random.rand() <= self.epsilon:\n",
        "              a = np.random.randint(0, self.n_action, size=1)[0]\n",
        "          else:\n",
        "              a = self.learned_act(s)\n",
        "        else:\n",
        "          a = self.learned_act(s)\n",
        "        return a\n",
        "\n",
        "    def learned_act(self,s):\n",
        "        \"\"\" Act via the policy of the agent, from a given state s\n",
        "        it proposes an action a\"\"\"\n",
        "        pass\n",
        "\n",
        "    def reinforce(self, s, n_s, a, r, game_over_):\n",
        "        \"\"\" This function is the core of the learning algorithm. \n",
        "        It takes as an input the current state s_, the next state n_s_\n",
        "        the action a_ used to move from s_ to n_s_ and the reward r_.\n",
        "        \n",
        "        Its goal is to learn a policy.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def save(self):\n",
        "        \"\"\" This function returns basic stats if applicable: the\n",
        "        loss and/or the model\"\"\"\n",
        "        pass\n",
        "\n",
        "    def load(self):\n",
        "        \"\"\" This function allows to restore a model\"\"\"\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e-ZetkZXxO2Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "***\n",
        "__Question 1__:\n",
        "Explain the function act. Why is ```epsilon``` essential?"
      ]
    },
    {
      "metadata": {
        "id": "5fPbd1_9xO2Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Epsilon is a threshold that decides how much the agent should explore or exploit what it has already learned"
      ]
    },
    {
      "metadata": {
        "id": "KrKnKeL1xO2b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "***\n",
        "### The Game"
      ]
    },
    {
      "metadata": {
        "id": "pVOwLXUAxO2d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The ```Agent``` and the ```Environment``` work in an interlaced way as in the following (take some time to understand this code as it is the core of the project)\n",
        "\n",
        "```python\n",
        "\n",
        "epoch = 300\n",
        "env = Environment()\n",
        "agent = Agent()\n",
        "\n",
        "\n",
        "# Number of won games\n",
        "score = 0\n",
        "loss = 0\n",
        "\n",
        "\n",
        "for e in range(epoch):\n",
        "    # At each epoch, we restart to a fresh game and get the initial state\n",
        "    state = env.reset()\n",
        "    # This assumes that the games will end\n",
        "    game_over = False\n",
        "\n",
        "    win = 0\n",
        "    lose = 0\n",
        "    \n",
        "    while not game_over:\n",
        "        # The agent performs an action\n",
        "        action = agent.act(state)\n",
        "\n",
        "        # Apply an action to the environment, get the next state, the reward\n",
        "        # and if the games end\n",
        "        prev_state = state\n",
        "        state, reward, game_over = env.act(action)\n",
        "\n",
        "        # Update the counters\n",
        "        if reward > 0:\n",
        "            win = win + reward\n",
        "        if reward < 0:\n",
        "            lose = lose -reward\n",
        "\n",
        "        # Apply the reinforcement strategy\n",
        "        loss = agent.reinforce(prev_state, state,  action, reward, game_over)\n",
        "\n",
        "    # Save as a mp4\n",
        "    if e % 10 == 0:\n",
        "        env.draw(e)\n",
        "\n",
        "    # Update stats\n",
        "    score += win-lose\n",
        "\n",
        "    print(\"Epoch {:03d}/{:03d} | Loss {:.4f} | Win/lose count {}/{} ({})\"\n",
        "          .format(e, epoch, loss, win, lose, win-lose))\n",
        "    agent.save()\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "hwgNuJ7CxO2g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# The game, *eat cheese*"
      ]
    },
    {
      "metadata": {
        "id": "cpSUnmRKxO2j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A rat runs on an island and tries to eat as much as possible. The island is subdivided into $N\\times N$ cells, in which there are cheese (+0.5) and poisonous cells (-1). The rat has a visibility of 2 cells (thus it can see $5^2$ cells). The rat is given a time $T$ to accumulate as much food as possible. It can perform 4 actions: going up, down, left, right. \n",
        "\n",
        "The goal is to code an agent to solve this task that will learn by trial and error. We propose the following environment:"
      ]
    },
    {
      "metadata": {
        "id": "CMean6c_xO2k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Environment(object):\n",
        "    def __init__(self, grid_size=10, max_time=500, temperature=0.1):\n",
        "        grid_size = grid_size+4\n",
        "        self.grid_size = grid_size\n",
        "        self.max_time = max_time\n",
        "        self.temperature = temperature\n",
        "\n",
        "        #board on which one plays\n",
        "        self.board = np.zeros((grid_size,grid_size))\n",
        "        self.position = np.zeros((grid_size,grid_size))\n",
        "\n",
        "        # coordinate of the rat\n",
        "        self.x = 0\n",
        "        self.y = 1\n",
        "\n",
        "        # self time\n",
        "        self.t = 0\n",
        "\n",
        "        self.scale=16\n",
        "\n",
        "        self.to_draw = np.zeros((max_time+2, grid_size*self.scale, grid_size*self.scale, 3))\n",
        "\n",
        "\n",
        "    def draw(self,e):\n",
        "        skvideo.io.vwrite(str(e) + '.mp4', self.to_draw)\n",
        "\n",
        "    def get_frame(self,t):\n",
        "        b = np.zeros((self.grid_size,self.grid_size,3))+128\n",
        "        b[self.board>0,0] = 256\n",
        "        b[self.board < 0, 2] = 256\n",
        "        b[self.x,self.y,:]=256\n",
        "        b[-2:,:,:]=0\n",
        "        b[:,-2:,:]=0\n",
        "        b[:2,:,:]=0\n",
        "        b[:,:2,:]=0\n",
        "        \n",
        "        b =  cv2.resize(b, None, fx=self.scale, fy=self.scale, interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "        self.to_draw[t,:,:,:]=b\n",
        "\n",
        "\n",
        "    def act(self, action):\n",
        "        \"\"\"This function returns the new state, reward and decides if the\n",
        "        game ends.\"\"\"\n",
        "\n",
        "        self.get_frame(int(self.t))\n",
        "\n",
        "        self.position = np.zeros((self.grid_size, self.grid_size))\n",
        "\n",
        "        self.position[0:2,:]= -1\n",
        "        self.position[:,0:2] = -1\n",
        "        self.position[-2:, :] = -1\n",
        "        #error : twice same line ?\n",
        "        self.position[:, -2:] = -1\n",
        "\n",
        "        self.position[self.x, self.y] = 1\n",
        "        if action == 0:\n",
        "            #Right\n",
        "            if self.x == self.grid_size-3:\n",
        "                self.x = self.x-1\n",
        "            else:\n",
        "                self.x = self.x + 1\n",
        "        elif action == 1:\n",
        "            #Left\n",
        "            if self.x == 2:\n",
        "                self.x = self.x+1\n",
        "            else:\n",
        "                self.x = self.x-1\n",
        "        elif action == 2:\n",
        "            #Up\n",
        "            if self.y == self.grid_size - 3:\n",
        "                self.y = self.y - 1\n",
        "            else:\n",
        "                self.y = self.y + 1\n",
        "        elif action == 3:\n",
        "            #Down\n",
        "            if self.y == 2:\n",
        "                self.y = self.y + 1\n",
        "            else:\n",
        "                self.y = self.y - 1\n",
        "        else:\n",
        "            RuntimeError('Error: action not recognized')\n",
        "\n",
        "        self.t = self.t + 1\n",
        "        reward = self.board[self.x, self.y]\n",
        "        self.board[self.x, self.y] = 0\n",
        "        game_over = self.t > self.max_time\n",
        "        state = np.concatenate((self.board.reshape(self.grid_size, self.grid_size,1),\n",
        "                        self.position.reshape(self.grid_size, self.grid_size,1)),axis=2)\n",
        "        state = state[self.x-2:self.x+3,self.y-2:self.y+3,:]\n",
        "\n",
        "        return state, reward, game_over\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"This function resets the game and returns the initial state\"\"\"\n",
        "\n",
        "        self.x = np.random.randint(3, self.grid_size-3, size=1)[0]\n",
        "        self.y = np.random.randint(3, self.grid_size-3, size=1)[0]\n",
        "\n",
        "\n",
        "        bonus = 0.5*np.random.binomial(1,self.temperature,size=self.grid_size**2)\n",
        "        bonus = bonus.reshape(self.grid_size,self.grid_size)\n",
        "\n",
        "        malus = -1.0*np.random.binomial(1,self.temperature,size=self.grid_size**2)\n",
        "        malus = malus.reshape(self.grid_size, self.grid_size)\n",
        "\n",
        "        self.to_draw = np.zeros((self.max_time+2, self.grid_size*self.scale, self.grid_size*self.scale, 3))\n",
        "\n",
        "\n",
        "        malus[bonus>0]=0\n",
        "\n",
        "        self.board = bonus + malus\n",
        "\n",
        "        self.position = np.zeros((self.grid_size, self.grid_size))\n",
        "        self.position[0:2,:]= -1\n",
        "        self.position[:,0:2] = -1\n",
        "        self.position[-2:, :] = -1\n",
        "        self.position[:, -2:] = -1\n",
        "        self.board[self.x,self.y] = 0\n",
        "        self.t = 0\n",
        "\n",
        "        state = np.concatenate((\n",
        "                               self.board.reshape(self.grid_size, self.grid_size,1),\n",
        "                        self.position.reshape(self.grid_size, self.grid_size,1)),axis=2)\n",
        "\n",
        "        state = state[self.x - 2:self.x + 3, self.y - 2:self.y + 3, :]\n",
        "        return state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XqkG_34WxO2q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The following elements are important because they correspond to the hyper parameters for this project:"
      ]
    },
    {
      "metadata": {
        "id": "o0kqfZ5VxO2r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# parameters\n",
        "size = 13\n",
        "T=150\n",
        "temperature=0.3\n",
        "epochs_train=200 # set small when debugging\n",
        "epochs_test=100 # set small when debugging\n",
        "\n",
        "# display videos\n",
        "def display_videos(name):\n",
        "    video = io.open(name, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    return '''<video alt=\"test\" controls>\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SC7OIOxLxO2y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__Question 2__ Explain the use of the arrays ```position``` and ```board```."
      ]
    },
    {
      "metadata": {
        "id": "2xf0PepNxO2y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Position is were the rat is (value:1) and where he isn't(value:-1)"
      ]
    },
    {
      "metadata": {
        "id": "uLRUvUohxO2z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Random Agent"
      ]
    },
    {
      "metadata": {
        "id": "0vEog7_AxO2z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "***\n",
        "__Question 3__ Implement a random Agent (only ```learned_act``` needs to be implemented):"
      ]
    },
    {
      "metadata": {
        "id": "l8ipfiRIxO20",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RandomAgent(Agent):\n",
        "    def __init__(self):\n",
        "        super(RandomAgent, self).__init__()\n",
        "        pass\n",
        "\n",
        "    def learned_act(self, s):\n",
        "        a = np.random.randint(0, self.n_action, size=1)[0]\n",
        "        return a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5Ekh-1__xO22",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "***\n",
        "***\n",
        "__Question 4__ Visualize the game moves. You need to fill in the following function for the evaluation:"
      ]
    },
    {
      "metadata": {
        "id": "8_bYTecTxO22",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test(agent,env,epochs,prefix=''):\n",
        "    # Number of won games\n",
        "    score = 0\n",
        "        \n",
        "    for e in range(epochs):\n",
        "        \n",
        "      # At each epoch, we restart to a fresh game and get the initial state\n",
        "      state = env.reset()\n",
        "      # This assumes that the games will end\n",
        "      game_over = False\n",
        "\n",
        "      win = 0\n",
        "      lose = 0\n",
        "\n",
        "      while not game_over:\n",
        "          # The agent performs an action\n",
        "          action = agent.act(state)\n",
        "\n",
        "          # Apply an action to the environment, get the next state, the reward\n",
        "          # and if the games end\n",
        "          prev_state = state\n",
        "          state, reward, game_over = env.act(action)\n",
        "          # Update the counters\n",
        "          if reward > 0:\n",
        "              win = win + reward\n",
        "          if reward < 0:\n",
        "              lose = lose -reward\n",
        "\n",
        "      # Save as a mp4\n",
        "      if e%5 == 0:\n",
        "        env.draw(prefix+str(e))\n",
        "      # Update stats\n",
        "      score = score + win-lose\n",
        "\n",
        "    print('Final score: '+str(score/epochs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rtgsH2vmxO25",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b05be1a9-5f8f-458f-9cf5-691773ec3850"
      },
      "cell_type": "code",
      "source": [
        "if testing:\n",
        "  # Initialize the game\n",
        "  env = Environment(grid_size=size, max_time=T,temperature=temperature)\n",
        "\n",
        "  # Initialize the agent!\n",
        "  agent = RandomAgent()\n",
        "\n",
        "  test(agent,env,epochs_test,prefix='random')\n",
        "  HTML(display_videos('random0.mp4'))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final score: -3.18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QD1xiSYFxO2-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "***\n",
        "## DQN"
      ]
    },
    {
      "metadata": {
        "id": "eL-nAP0qxO2_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let us assume here that $T=\\infty$.\n",
        "\n",
        "***\n",
        "__Question 5__ Let $\\pi$ be a policy, show that:\n",
        "\n",
        "\\begin{equation*}\n",
        "Q^{\\pi}(s,a)=E_{(s',a')\\sim p(.|s,a)}[r(s,a)+\\gamma Q^{\\pi}(s',a')]\n",
        "\\end{equation*}\n",
        "\n",
        "Let $Gt = \\sum_{k=0}^{\\infty}{\\gamma^kR_{t+k+1}}$ be the return \n",
        "\n",
        "$Q^{\\pi}(s,a)=E_{\\pi}[G_{t} | S_{t}=s, A_{t} =a]$\n",
        "\n",
        "$Q^{\\pi}(s,a)=E_{\\pi}[r(s,a) + \\sum_{k=1}^{\\infty}{\\gamma^kR_{t+k+1}} | S_{t}=s, A_{t} =a]$\n",
        "$Q^{\\pi}(s,a)=E_{\\pi}[r(s,a) + \\gamma\\sum_{k=1}^{\\infty}{\\gamma^{k-1}R_{t+k+1}} | S_{t}=s, A_{t} =a]$\n",
        "$Q^{\\pi}(s,a)=E_{\\pi}[r(s,a) + \\gamma\\sum_{k=0}^{\\infty}{\\gamma^{k}R_{t+k+2}} | S_{t}=s, A_{t} =a]$\n",
        "$Q^{\\pi}(s,a)=E_{\\pi}[r(s,a) + \\gamma G_{t+1} | S_{t}=s, A_{t} =a]$\n",
        "$Q^{\\pi}(s,a)=E_{\\pi}[r(s,a) + \\gamma E_{\\pi}[G_{t+1}  | S_{t}=s', A_{t} =a'] | S_{t}=s, A_{t} =a]$\n",
        "$Q^{\\pi}(s,a)=E_{\\pi}[r(s,a) + \\gamma Q^{\\pi}(s',a')| S_{t}=s, A_{t} =a]$\n",
        "$Q^{\\pi}(s,a)=E_{(s',a')\\sim p(.|s,a)}[r(s,a)+\\gamma Q^{\\pi}(s',a')]$\n",
        "\n",
        "Then, show that for the optimal policy $\\pi^*$ (we assume its existence), the following holds: \n",
        "\n",
        "\\begin{equation*}\n",
        "Q^{*}(s,a)=E_{s'\\sim \\pi^*(.|s,a)}[r(s,a)+\\gamma\\max_{a'}Q^{*}(s',a')].\n",
        "\\end{equation*}\n",
        "\n",
        "\n",
        "$Q^{\\pi}(s,a)= \\max_{\\pi}{E_{(s',a')\\sim p(.|s,a)}[r(s,a)+\\gamma Q^{\\pi}(s',a')]}$\n",
        "$Q^{\\pi}(s,a)= r(s,a) + \\gamma \\sum_{a' \\in A} {\\pi(a'|s')Q^{\\pi}(s′,a′)} $\n",
        "\n",
        "$Q^{*}(s,a)= \\max_{\\pi}{Q^{\\pi}(s,a)} $\n",
        "\n",
        "$Q^{*}(s,a)= \\max_{\\pi}{r(s,a) + \\gamma \\sum_{a' \\in A} {\\pi(a'|s')Q^{\\pi}(s′,a′) }}$\n",
        "\n",
        "$Q^{*}(s,a)= r(s,a) + \\gamma \\sum_{a' \\in A} {max_{\\pi}{\\pi(a'|s')Q^{\\pi}(s′,a′)}} $\n",
        "\n",
        "Let consider : $\\pi'(s) = argmax_{a \\in A}{Q^{\\pi}(s,a)}$\n",
        "For any policy $\\pi$ , $ Q^{\\pi}(s,a) \\le max_{a \\in A}{Q^{\\pi}(s,a)} =  Q^{\\pi}(s,\\pi'(s)) \\le Q^{*}(s,\\pi'(s))$\n",
        "\n",
        "The max is obtained by folowing the greedy policy.\n",
        "\n",
        "Therefore :\n",
        "\n",
        "$Q^{*}(s,a)= r(s,a) + \\gamma  Q^{*}(s′,\\pi'(s')) $\n",
        "$Q^{*}(s,a)= r(s,a) + \\gamma\\max_{a'}Q^{*}(s',a') $\n",
        "\n",
        "\n",
        "\n",
        "Finally, deduce that a plausible objective is:\n",
        "\n",
        "\\begin{equation*}\n",
        "\\mathcal{L}(\\theta)=E_{s' \\sim \\pi^*(.|s,a)}\\Vert r+\\gamma\\max\\max_{a'}Q(s',a',\\theta)-Q(s,a,\\theta)\\Vert^{2}.\n",
        "\\end{equation*}\n",
        "\n",
        "We have a neural network that approximates the state value function Q(s,a,$\\theta$), and an oracle of the best posible action to take : $\\gamma r+ max_{a'}Q(s',a',\\theta)$ At each step we minimise the mean squared error.\n"
      ]
    },
    {
      "metadata": {
        "id": "3xWA8PMWxO3B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "***\n",
        "The DQN-learning algorithm relies on these derivations to train the parameters $\\theta$ of a Deep Neural Network:\n",
        "\n",
        "1. At the state $s_t$, select the action $a_t$ with best reward using $Q_t$ and store the results;\n",
        "\n",
        "2. Obtain the new state $s_{t+1}$ from the environment $p$;\n",
        "\n",
        "3. Store $(s_t,a_t,s_{t+1})$;\n",
        "\n",
        "4. Obtain $Q_{t+1}$ by minimizing  $\\mathcal{L}$ from a recovered batch from the previously stored results.\n",
        "\n",
        "***\n",
        "__Question 6__ Implement the class ```Memory``` that stores moves (in a replay buffer) via ```remember``` and provides a ```random_access``` to these. Specify a maximum memory size to avoid side effects. You can for example use a ```list()``` and set by default ```max_memory=100```."
      ]
    },
    {
      "metadata": {
        "id": "eDyowBX-xO3C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Memory(object):\n",
        "    def __init__(self, max_memory=100):\n",
        "        self.max_memory = max_memory\n",
        "        self.memory = np.array([]).reshape(0,5)\n",
        "\n",
        "    def remember(self, m):\n",
        "        if self.memory.shape[0]> self.max_memory:\n",
        "          self.memory = np.delete(self.memory, (np.random.randint(0, self.max_memory)), axis=0)\n",
        "        self.memory = np.vstack([self.memory, m])\n",
        "        \n",
        "          \n",
        "    def random_access(self, batch_size):\n",
        "        idxs = np.random.randint(0, self.memory.shape[0], size=batch_size)\n",
        "        return self.memory[idxs,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2iR5LhfvxO3K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "***\n",
        "The pipeline we will use for training is given below:"
      ]
    },
    {
      "metadata": {
        "id": "D7gsEoC3xO3M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(agent,env,epoch,prefix=''):\n",
        "    # Number of won games\n",
        "    score = 0\n",
        "    loss = 0\n",
        "    sum_win = 0\n",
        "    sum_lose = 0\n",
        "    sum_loss = 0\n",
        "    for e in range(epoch):\n",
        "        # At each epoch, we restart to a fresh game and get the initial state\n",
        "        state = env.reset()\n",
        "        # This assumes that the games will terminate\n",
        "        game_over = False\n",
        "\n",
        "        win = 0\n",
        "        lose = 0\n",
        "\n",
        "        while not game_over:\n",
        "            # The agent performs an action\n",
        "            action = agent.act(state)\n",
        "\n",
        "            # Apply an action to the environment, get the next state, the reward\n",
        "            # and if the games end\n",
        "            prev_state = state\n",
        "            state, reward, game_over = env.act(action)\n",
        "\n",
        "            # Update the counters\n",
        "            if reward > 0:\n",
        "                win = win + reward\n",
        "            if reward < 0:\n",
        "                lose = lose -reward\n",
        "\n",
        "            # Apply the reinforcement strategy\n",
        "            loss = agent.reinforce(prev_state, state,  action, reward, game_over)\n",
        "\n",
        "        # Save as a mp4\n",
        "        if e % 20 == 0:\n",
        "            env.draw(prefix+str(e))\n",
        "\n",
        "        # Update stats\n",
        "        score += win-lose\n",
        "        sum_win += win\n",
        "        sum_lose += lose\n",
        "        sum_loss += loss\n",
        "\n",
        "        if e %10 == 0:\n",
        "          print(\"Epoch {:03d}/{:03d} | Loss {:.4f} | Win/lose count {}/{} ({})\"\n",
        "                .format(e, epoch, sum_loss/10, sum_win/10, sum_lose/10, (sum_win-sum_lose)/10))\n",
        "          agent.save(name_weights=prefix+'model.h5',name_model=prefix+'model.json')\n",
        "          sum_win = 0\n",
        "          sum_lose = 0\n",
        "          sum_loss = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qzizfN_QxO3Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "***\n",
        "__Question 7__ Implement the DQN training algorithm using a cascade of fully connected layers. You can use different learning rate, batch size or memory size parameters. In particular, the loss might oscillate while the player will start to win the games. You have to find a good criterium."
      ]
    },
    {
      "metadata": {
        "id": "th_XODm4xO3R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DQN(Agent):\n",
        "    def __init__(self, grid_size,  epsilon = 0.1, memory_size=100, batch_size = 16,n_state=2):\n",
        "        super(DQN, self).__init__(epsilon = epsilon)\n",
        "\n",
        "        # Discount for Q learning\n",
        "        self.discount = 0.99\n",
        "        \n",
        "        self.grid_size = grid_size\n",
        "        \n",
        "        # number of state\n",
        "        self.n_state = n_state\n",
        "\n",
        "        # Memory\n",
        "        self.memory = Memory(memory_size)\n",
        "        \n",
        "        # Batch size when learning\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def learned_act(self, s):\n",
        "        return np.argmax(self.model.predict(np.reshape(s, [1,5,5, self.n_state]))[0])\n",
        "\n",
        "    def reinforce(self, s_, n_s_, a_, r_, game_over_):\n",
        "        # Two steps: first memorize the states, second learn from the pool\n",
        "        self.memory.remember([np.reshape(s_, [1,5,5, self.n_state]), np.reshape(n_s_, [1,5,5, self.n_state]), a_, r_, game_over_])\n",
        "        \n",
        "        input_states = np.zeros((self.batch_size,5,5, self.n_state))\n",
        "        memory = self.memory.random_access(self.batch_size)\n",
        "        target_q = np.zeros((self.batch_size, self.n_action))\n",
        "        i=0\n",
        "        for s_, n_s_, a_, r_, game_over_ in memory:\n",
        "            input_states[i,:,:,:] = s_\n",
        "            ######## FILL IN\n",
        "            if game_over_:\n",
        "                target =  r_\n",
        "            else:\n",
        "                target = r_ + self.discount * \\\n",
        "                          np.max(self.model.predict(n_s_)[0])\n",
        "            target_q[i,:] = self.model.predict(s_)\n",
        "            target_q[i,:][a_] = target\n",
        "            i +=1 \n",
        "        ######## FILL IN\n",
        "        # HINT: Clip the target to avoid exploiding gradients.. -- clipping is a bit tighter\n",
        "        target_q = np.clip(target_q, -3, 3)\n",
        "\n",
        "        l = self.model.train_on_batch(input_states, target_q)\n",
        "\n",
        "        return l\n",
        "\n",
        "    def save(self,name_weights='model.h5',name_model='model.json'):\n",
        "        self.model.save_weights(name_weights, overwrite=True)\n",
        "        with open(name_model, \"w\") as outfile:\n",
        "            json.dump(self.model.to_json(), outfile)\n",
        "            \n",
        "    def load(self,name_weights='model.h5',name_model='model.json'):\n",
        "        with open(name_model, \"r\") as jfile:\n",
        "            model = model_from_json(json.load(jfile))\n",
        "        model.load_weights(name_weights)\n",
        "        model.compile(\"sgd\", \"mse\")\n",
        "        self.model = model\n",
        "\n",
        "            \n",
        "class DQN_FC(DQN):\n",
        "    def __init__(self, *args, lr=0.1,**kwargs):\n",
        "        super(DQN_FC, self).__init__( *args,**kwargs)\n",
        "        \n",
        "        model = Sequential()\n",
        "        model.add(Reshape((5*5*self.n_state,), input_shape=(5,5,self.n_state)))\n",
        "        model.add(Dense(32, activation='relu'))\n",
        "        model.add(Dense(12, activation='relu'))\n",
        "        model.add(Dense(self.n_action, activation='linear'))\n",
        "        model.compile(sgd(lr=lr, decay=1e-4, momentum=0.0), \"mse\")\n",
        "        self.model = model\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6ml3c8CpxO3U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if training:\n",
        "  env = Environment(grid_size=size, max_time=T, temperature=0.3)\n",
        "  agent = DQN_FC(size, lr=.1, epsilon = 0.1, memory_size=1000, batch_size = 32)\n",
        "  train(agent, env, epochs_train, prefix='fc_train')\n",
        "  HTML(display_videos('fc_train20.mp4'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l4YZc0e2xO3V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "***\n",
        "***\n",
        "__Question 8__ Implement the DQN training algorithm using a CNN (for example, 2 convolutional layers and one final fully connected layer)."
      ]
    },
    {
      "metadata": {
        "id": "995JTycAxO3X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DQN_CNN(DQN):\n",
        "    def __init__(self, *args,lr=0.1,**kwargs):\n",
        "        super(DQN_CNN, self).__init__(*args,**kwargs)\n",
        "        \n",
        "        model = Sequential()\n",
        "        model.add(Conv2D(16, 8, 8, border_mode='same', input_shape=(5,5,self.n_state)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(Conv2D(32, 4, 4, border_mode='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(256, activation='relu'))\n",
        "        model.add(Dense(self.n_action))\n",
        "        model.compile(sgd(lr=lr, decay=1e-4, momentum=0.0), \"mse\")\n",
        "        self.model = model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h3GI1qftxO3a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if training:\n",
        "  env = Environment(grid_size=size, max_time=T, temperature=0.3)\n",
        "  agent = DQN_CNN(size, lr=.1, epsilon = 0.1, memory_size=1000, batch_size = 32)\n",
        "  train(agent,env,epochs_train,prefix='cnn_train')\n",
        "  HTML(display_videos('cnn_train20.mp4'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SIv3FUTOxO3d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "***\n",
        "***\n",
        "__Question 9__ Test both algorithms and compare their performances. Which issue(s) do you observe? Observe also different behaviors by changing the temperature."
      ]
    },
    {
      "metadata": {
        "id": "Uq1dV5nRxO3d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "201a38c8-01c6-4403-835c-7d508dcbf1bf"
      },
      "cell_type": "code",
      "source": [
        "if testing:\n",
        "  env = Environment(grid_size=size, max_time=T,temperature=0.3)\n",
        "  agent_cnn = DQN_CNN(size, lr=.1, epsilon = 0.1, memory_size=2000, batch_size = 32)\n",
        "  agent_cnn.load(name_weights='cnn_trainmodel.h5',name_model='cnn_trainmodel.json')\n",
        "\n",
        "  agent_fc = DQN_FC(size, lr=.1, epsilon = 0.1, memory_size=2000, batch_size = 32)\n",
        "  agent_fc.load(name_weights='fc_trainmodel.h5',name_model='fc_trainmodel.json')\n",
        "  print('Test of the CNN')\n",
        "  test(agent_cnn,env,epochs_test,prefix='cnn_test')\n",
        "  print('Test of the FC')\n",
        "  test(agent_fc,env,epochs_test,prefix='fc_test')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (8, 8), input_shape=(5, 5, 2), padding=\"same\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (4, 4), padding=\"same\")`\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test of the CNN\n",
            "Final score: 7.865\n",
            "Test of the FC\n",
            "Final score: 10.96\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7gFAayV9xO3f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "8fa260a5-0dcb-42fc-8be2-5969dad2b15e"
      },
      "cell_type": "code",
      "source": [
        "HTML(display_videos('cnn_test20.mp4'))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<video alt=\"test\" controls>\n",
              "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "             </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "m2D_1aj1xO3i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "a59054a8-c303-494a-9c32-fc58ecde2141"
      },
      "cell_type": "code",
      "source": [
        "HTML(display_videos('fc_test20.mp4'))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<video alt=\"test\" controls>\n",
              "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "             </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "metadata": {
        "id": "UR-fqEgrxO3m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "***\n",
        "\n",
        "The algorithm tends to not explore the map which can be an issue. We propose two ideas in order to encourage exploration:\n",
        "1. Incorporating a decreasing $\\epsilon$-greedy exploration. You can use the method ```set_epsilon```\n",
        "2. Append via the environment a new state that describes if a cell has been visited or not\n",
        "\n",
        "***\n",
        "__Question 10__ Design a new ```train_explore``` function and environment class ```EnvironmentExploring``` to tackle the issue of exploration.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "mqsRgkQkxO3o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_explore(agent,env,epoch,prefix=''):\n",
        "    # Number of won games\n",
        "    score = 0\n",
        "    loss = 0\n",
        "    sum_win = 0\n",
        "    sum_lose = 0\n",
        "    sum_loss = 0\n",
        "    for e in range(epoch):\n",
        "        # At each epoch, we restart to a fresh game and get the initial state\n",
        "        state = env.reset()\n",
        "        # This assumes that the games will terminate\n",
        "        game_over = False\n",
        "\n",
        "        win = 0\n",
        "        lose = 0\n",
        "\n",
        "        while not game_over:\n",
        "            # The agent performs an action\n",
        "            action = agent.act(state, train=True)\n",
        "\n",
        "            # Apply an action to the environment, get the next state, the reward\n",
        "            # and if the games end\n",
        "            prev_state = state\n",
        "            state, reward, game_over = env.act(action, train=True)\n",
        "\n",
        "            # Update the counters\n",
        "            if reward > 0:\n",
        "                win = win + reward\n",
        "            if reward < 0:\n",
        "                lose = lose -reward\n",
        "\n",
        "            # Apply the reinforcement strategy\n",
        "            loss = agent.reinforce(prev_state, state,  action, reward, game_over)\n",
        "\n",
        "        if e%50 == 0:\n",
        "          agent.set_epsilon(max(agent.epsilon/1.5,0.1))\n",
        "        # Save as a mp4\n",
        "        if e % 20 == 0:\n",
        "            env.draw(prefix+str(e))\n",
        "\n",
        "        # Update stats\n",
        "        score += win-lose\n",
        "        sum_win += win\n",
        "        sum_lose += lose\n",
        "        sum_loss += loss\n",
        "\n",
        "        if e %10 == 0:\n",
        "          print(\"Epoch {:03d}/{:03d} | Loss {:.4f} | Win/lose count {}/{} ({})\"\n",
        "                .format(e, epoch, sum_loss/10, sum_win/10, sum_lose/10, (sum_win-sum_lose)/10))\n",
        "          agent.save(name_weights=prefix+'model.h5',name_model=prefix+'model.json')\n",
        "          sum_win = 0\n",
        "          sum_lose = 0\n",
        "          sum_loss = 0\n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ugnBB1MhaTzz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class EnvironmentExploring(object):\n",
        "    def __init__(self, grid_size=10, max_time=500, temperature=0.1):\n",
        "        grid_size = grid_size+4\n",
        "        self.grid_size = grid_size\n",
        "        self.max_time = max_time\n",
        "        self.temperature = temperature\n",
        "\n",
        "        #board on which one plays\n",
        "        self.board = np.zeros((grid_size,grid_size))\n",
        "        self.position = np.zeros((grid_size,grid_size))\n",
        "        self.malus_position = np.zeros((grid_size,grid_size))\n",
        "        # coordinate of the rat\n",
        "        self.x = 0\n",
        "        self.y = 1\n",
        "\n",
        "        # self time\n",
        "        self.t = 0\n",
        "\n",
        "        self.scale=16\n",
        "\n",
        "        self.to_draw = np.zeros((max_time+2, grid_size*self.scale, grid_size*self.scale, 3))\n",
        "\n",
        "\n",
        "    def draw(self,e):\n",
        "        skvideo.io.vwrite(str(e) + '.mp4', self.to_draw)\n",
        "\n",
        "    def get_frame(self,t):\n",
        "        b = np.zeros((self.grid_size,self.grid_size,3))+128\n",
        "        b[self.board>0,0] = 256\n",
        "        b[self.board < 0, 2] = 256\n",
        "        b[self.x,self.y,:]=256\n",
        "        b[-2:,:,:]=0\n",
        "        b[:,-2:,:]=0\n",
        "        b[:2,:,:]=0\n",
        "        b[:,:2,:]=0\n",
        "        \n",
        "        b =  cv2.resize(b, None, fx=self.scale, fy=self.scale, interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "        self.to_draw[t,:,:,:]=b\n",
        "\n",
        "\n",
        "    def act(self, action, train=False):\n",
        "        \"\"\"This function returns the new state, reward and decides if the\n",
        "        game ends.\"\"\"\n",
        "\n",
        "        self.get_frame(int(self.t))\n",
        "\n",
        "        self.position = np.zeros((self.grid_size, self.grid_size))\n",
        "        \n",
        "        self.position[0:2,:]= -1\n",
        "        self.position[:,0:2] = -1\n",
        "        self.position[-2:, :] = -1\n",
        "        #error : twice same line ?\n",
        "        self.position[:, -2:] = -1\n",
        "\n",
        "        self.position[self.x, self.y] = 1\n",
        "        if action == 0:\n",
        "            #Right\n",
        "            if self.x == self.grid_size-3:\n",
        "                self.x = self.x-1\n",
        "            else:\n",
        "                self.x = self.x + 1\n",
        "        elif action == 1:\n",
        "            #Left\n",
        "            if self.x == 2:\n",
        "                self.x = self.x+1\n",
        "            else:\n",
        "                self.x = self.x-1\n",
        "        elif action == 2:\n",
        "            #Up\n",
        "            if self.y == self.grid_size - 3:\n",
        "                self.y = self.y - 1\n",
        "            else:\n",
        "                self.y = self.y + 1\n",
        "        elif action == 3:\n",
        "            #Down\n",
        "            if self.y == 2:\n",
        "                self.y = self.y + 1\n",
        "            else:\n",
        "                self.y = self.y - 1\n",
        "        else:\n",
        "            RuntimeError('Error: action not recognized')\n",
        "\n",
        "        self.t = self.t + 1\n",
        "        reward = 0\n",
        "        if train:\n",
        "            reward = -self.malus_position[self.x, self.y]\n",
        "        self.malus_position[self.x, self.y] = 0.1\n",
        "\n",
        "        reward = reward + self.board[self.x, self.y]\n",
        "        self.board[self.x, self.y] = 0\n",
        "        game_over = self.t > self.max_time\n",
        "        state = np.concatenate((self.malus_position.reshape(self.grid_size, self.grid_size,1),\n",
        "                                self.board.reshape(self.grid_size, self.grid_size,1),\n",
        "                        self.position.reshape(self.grid_size, self.grid_size,1)),axis=2)\n",
        "        state = state[self.x-2:self.x+3,self.y-2:self.y+3,:]\n",
        "\n",
        "        return state, reward, game_over\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"This function resets the game and returns the initial state\"\"\"\n",
        "\n",
        "        self.x = np.random.randint(3, self.grid_size-3, size=1)[0]\n",
        "        self.y = np.random.randint(3, self.grid_size-3, size=1)[0]\n",
        "\n",
        "\n",
        "        bonus = 0.5*np.random.binomial(1,self.temperature,size=self.grid_size**2)\n",
        "        bonus = bonus.reshape(self.grid_size,self.grid_size)\n",
        "\n",
        "        malus = -1.0*np.random.binomial(1,self.temperature,size=self.grid_size**2)\n",
        "        malus = malus.reshape(self.grid_size, self.grid_size)\n",
        "\n",
        "        self.to_draw = np.zeros((self.max_time+2, self.grid_size*self.scale, self.grid_size*self.scale, 3))\n",
        "\n",
        "\n",
        "        malus[bonus>0]=0\n",
        "\n",
        "        self.board = bonus + malus\n",
        "        self.malus_position = np.zeros((self.grid_size,self.grid_size))\n",
        "        self.position = np.zeros((self.grid_size, self.grid_size))\n",
        "        self.position[0:2,:]= -1\n",
        "        self.position[:,0:2] = -1\n",
        "        self.position[-2:, :] = -1\n",
        "        self.position[:, -2:] = -1\n",
        "        self.board[self.x,self.y] = 0\n",
        "        self.t = 0\n",
        "        # 3 \"feature\" states instead of 2\n",
        "        state = np.concatenate((self.malus_position.reshape(self.grid_size, self.grid_size,1),\n",
        "                                self.board.reshape(self.grid_size, self.grid_size,1),\n",
        "                        self.position.reshape(self.grid_size, self.grid_size,1)),axis=2)\n",
        "        state = state[self.x - 2:self.x + 3, self.y - 2:self.y + 3, :]\n",
        "        return state\n",
        "## In Environment exploring:\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KZqIUThZD6D5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if training:\n",
        "  env = Environment(grid_size=size, max_time=T, temperature=0.3)\n",
        "  agent = DQN_CNN(size, lr=.1, epsilon = 1, memory_size=1000, batch_size = 32)\n",
        "  train_explore(agent,env,epochs_train,prefix='cnn_train')\n",
        "  HTML(display_videos('cnn_train20.mp4'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bPrn7VGKXUel",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "1ea55417-ee48-4ca2-95c3-4134e528aff2"
      },
      "cell_type": "code",
      "source": [
        "if testing:\n",
        "  env = Environment(grid_size=size, max_time=T,temperature=0.3)\n",
        "  agent_cnn = DQN_CNN(size, lr=.1, epsilon = 0.1, memory_size=2000, batch_size = 32)\n",
        "  agent_cnn.load(name_weights='cnn_trainepsilonmodel.h5',name_model='cnn_trainepsilonmodel.json')\n",
        "  print('Test of the CNN')\n",
        "  test(agent_cnn,env,epochs_test,prefix='cnn_test')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (8, 8), input_shape=(5, 5, 2), padding=\"same\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (4, 4), padding=\"same\")`\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test of the CNN\n",
            "Final score: 11.87\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ebIz9VwIxO3u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Training\n",
        "# You will have to change n_state to 3 because you will use one more layer!\n",
        "if training:\n",
        "  env = EnvironmentExploring(grid_size=size, max_time=T, temperature=0.3)\n",
        "  agent = DQN_CNN(size, lr=.1, epsilon = 0.4, memory_size=1000, batch_size = 32,n_state=3)\n",
        "  train_explore(agent, env, epochs_train, prefix='cnn_train_explore')\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tzPyJNiiIp_w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "463bdd36-2176-49a3-d4e5-73df1abb29ed"
      },
      "cell_type": "code",
      "source": [
        "HTML(display_videos('cnn_train_explore20.mp4'))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<video alt=\"test\" controls>\n",
              "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "             </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "3zBdu2rLxO3w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "08041697-067d-4f7b-9152-7bfd8f581968"
      },
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "if testing:\n",
        "  env = EnvironmentExploring(grid_size=size, max_time=T, temperature=0.3)\n",
        "  agent = DQN_CNN(size, lr=.1, epsilon = 0.1, memory_size=2000, batch_size = 32,n_state=3)\n",
        "  agent.load(name_weights='cnn_train_exploremodel.h5',name_model='cnn_train_exploremodel.json')\n",
        "  print('Test of the CNN')\n",
        "  test(agent,env,epochs_test,prefix='cnn_test_explore')\n",
        "  HTML(display_videos('cnn_test_explore20.mp4'))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (8, 8), input_shape=(5, 5, 3), padding=\"same\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (4, 4), padding=\"same\")`\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test of the CNN\n",
            "Final score: 12.505\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Gk8bAMeXxO3y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "***\n",
        "***\n",
        "__BONUS question__ Use the expert DQN from the previous question to generate some winning games. Train a model that mimicks its behavior. Compare the performances."
      ]
    },
    {
      "metadata": {
        "id": "UGlgh3CPxO3z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "***"
      ]
    }
  ]
}